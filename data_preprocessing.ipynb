{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import logging\n",
    "\n",
    "\n",
    "# Generating dataset from csv file. Returns a Pandas DataFrame\n",
    "def entire_dataset_from_single_file(filename,\n",
    "                                    col_names,\n",
    "                                    selected_col_names,\n",
    "                                    remove_zero_req_prb_entries=True,\n",
    "                                    scale_dl_buffer=True,\n",
    "                                    replace_zero_with_one=False,\n",
    "                                    add_prb_ratio=True):\n",
    "    dataset = pd.read_csv(filename, names=col_names, usecols=selected_col_names, header=0)\n",
    "\n",
    "    if remove_zero_req_prb_entries:\n",
    "        dataset = dataset.loc[dataset['sum_requested_prbs'] > 0].reset_index(drop=True)\n",
    "\n",
    "    if scale_dl_buffer and any([\"dl_buffer [bytes]\" in m for m in\n",
    "                                selected_col_names]):\n",
    "        # scale the dl_buffer\n",
    "        dataset['dl_buffer [bytes]'] = dataset['dl_buffer [bytes]'] / 100000\n",
    "\n",
    "    if add_prb_ratio:\n",
    "        dict_add = pd.DataFrame.from_dict({\"ratio_granted_req\": np.clip(np.nan_to_num(\n",
    "            dataset[\"sum_granted_prbs\"] / dataset[\"sum_requested_prbs\"]), a_min=0, a_max=1)\n",
    "        })\n",
    "        if replace_zero_with_one:\n",
    "            dict_add['ratio_granted_req'].loc[dataset['sum_requested_prbs'] <= 0] = 1.0\n",
    "        return dataset.join(dict_add)\n",
    "    else:\n",
    "        return dataset\n",
    "\n",
    "# return all csv files inside a single DataFrame\n",
    "def entire_dataset_from_folder(main_folder,\n",
    "                               wildcard,\n",
    "                               col_names,\n",
    "                               selected_col_names,\n",
    "                               scale_dl_buffer=True,\n",
    "                               remove_zero_req_prb_entries=True,\n",
    "                               replace_zero_with_one=False,\n",
    "                               add_prb_ratio=True):\n",
    "    dataset = []\n",
    "    for filename in glob.glob(main_folder + wildcard):\n",
    "        db_tmp = entire_dataset_from_single_file(filename, col_names=col_names,\n",
    "                                                 selected_col_names=selected_col_names,\n",
    "                                                 scale_dl_buffer=scale_dl_buffer,\n",
    "                                                 remove_zero_req_prb_entries=remove_zero_req_prb_entries,\n",
    "                                                 replace_zero_with_one=replace_zero_with_one,\n",
    "                                                 add_prb_ratio=add_prb_ratio)\n",
    "        dataset.append(db_tmp)\n",
    "\n",
    "    return pd.concat(dataset, axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "# take n entries from the DataFrame at random\n",
    "def extract_n_entries_from_dataset(dataset=None,\n",
    "                                   slice_id=None,\n",
    "                                   n_entries=10,\n",
    "                                   metrics_export=None):\n",
    "    if slice_id is not None:\n",
    "        d_temp = dataset.loc[dataset['slice_id'] == int(slice_id)]\n",
    "    else:\n",
    "        d_temp = dataset\n",
    "\n",
    "    d_temp = d_temp.sample(n=n_entries).reset_index(drop=True)\n",
    "    if metrics_export is not None:\n",
    "        d_temp = d_temp[metrics_export]\n",
    "\n",
    "    return d_temp\n",
    "\n",
    "# This function is used here to emulate a DU reporting real-time data. Replace this function with your DU\n",
    "# FOR TESTING PURPOSES ONLY\n",
    "def get_data_from_DUs(dataset=None,\n",
    "                      n_entries=1000,\n",
    "                      n_col=4,\n",
    "                      slice_id=None,\n",
    "                      metrics_export=None):\n",
    "\n",
    "    if dataset is None:  # generate random data in case you do not have a dataset\n",
    "        values = np.random.random(size=(n_entries, n_col))\n",
    "        slice_id = np.random.randint(low=0, high=3, size=(n_entries, 1))\n",
    "        data = np.concatenate((slice_id, values), axis=1)\n",
    "    else:\n",
    "        data = extract_n_entries_from_dataset(dataset=dataset,\n",
    "                                              slice_id=slice_id,\n",
    "                                              n_entries=n_entries,\n",
    "                                              metrics_export=metrics_export)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Return lists for metrics, rewards, prbs assigned to each slice.\n",
    "# Ideally, the list is such that len(list) = num_slices\n",
    "def split_data(slice_profiles=None,\n",
    "               data_to_spit=None,\n",
    "               metric_list=None,\n",
    "               metric_dict=None,\n",
    "               n_entries_per_slice=None):\n",
    "    metrics = []\n",
    "    prbs = []\n",
    "    rewards = []\n",
    "\n",
    "    # ordering here follows slice_profiles\n",
    "    for i in slice_profiles:\n",
    "\n",
    "        slice_data = data_to_spit[data_to_spit[:, metric_dict['slice_id']] == slice_profiles[i]['slice_id'], :]\n",
    "\n",
    "        if slice_data.size > 0:\n",
    "            # repmat on rows to reach needed dimension in case you do not have enough reporting data\n",
    "            while slice_data.shape[0] < n_entries_per_slice:\n",
    "                slice_data = np.vstack((slice_data, np.zeros((1, slice_data.shape[1]))))\n",
    "\n",
    "            slice_prb = slice_data[:, metric_dict['slice_prb']]\n",
    "            slice_metrics = slice_data[:, [metric_dict[x] for x in metric_list]]\n",
    "            slice_reward = slice_data[:, metric_dict[slice_profiles[i]['reward_metric']]]\n",
    "\n",
    "            if n_entries_per_slice is not None:\n",
    "                slice_prb = slice_prb[0:n_entries_per_slice]\n",
    "                slice_metrics = slice_metrics[0:n_entries_per_slice, :]\n",
    "                slice_reward = slice_reward[0:n_entries_per_slice]\n",
    "        else:\n",
    "            slice_metrics = []\n",
    "            slice_prb = []\n",
    "            slice_reward = []\n",
    "\n",
    "        metrics.append(slice_metrics)\n",
    "        prbs.append(slice_prb)\n",
    "        rewards.append(slice_reward)\n",
    "\n",
    "    return metrics, prbs, rewards\n",
    "\n",
    "# Used to generate the input to the DRL agent. It returns a TimeStep that contains (step_type, reward, discount, observations)\n",
    "def generate_timestep_for_policy(obs_tmp=None):\n",
    "    step_type = tf.convert_to_tensor(\n",
    "        [0], dtype=tf.int32, name='step_type')\n",
    "    reward = tf.convert_to_tensor(\n",
    "        [0], dtype=tf.float32, name='reward')\n",
    "    discount = tf.convert_to_tensor(\n",
    "        [1], dtype=tf.float32, name='discount')\n",
    "    observations = tf.convert_to_tensor(\n",
    "        [obs_tmp], dtype=tf.float32, name='observations')\n",
    "    return ts.TimeStep(step_type, reward, discount, observations)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Column names in the srsLTE CSV dataset\n",
    "    all_metrics_list = [\"Timestamp\",\n",
    "                        \"num_ues\",\n",
    "                        \"IMSI\",\n",
    "                        \"RNTI\",\n",
    "                        \"empty_1\",\n",
    "                        \"slicing_enabled\",\n",
    "                        \"slice_id\",\n",
    "                        \"slice_prb\",\n",
    "                        \"power_multiplier\",\n",
    "                        \"scheduling_policy\",\n",
    "                        \"empty_2\",\n",
    "                        \"dl_mcs\",\n",
    "                        \"dl_n_samples\",\n",
    "                        \"dl_buffer [bytes]\",\n",
    "                        \"tx_brate downlink [Mbps]\",\n",
    "                        \"tx_pkts downlink\",\n",
    "                        \"tx_errors downlink (%)\",\n",
    "                        \"dl_cqi\",\n",
    "                        \"empty_3\",\n",
    "                        \"ul_mcs\",\n",
    "                        \"ul_n_samples\",\n",
    "                        \"ul_buffer [bytes]\",\n",
    "                        \"rx_brate uplink [Mbps]\",\n",
    "                        \"rx_pkts uplink\",\n",
    "                        \"rx_errors uplink (%)\",\n",
    "                        \"ul_rssi\",\n",
    "                        \"ul_sinr\",\n",
    "                        \"phr\",\n",
    "                        \"empty_4\",\n",
    "                        \"sum_requested_prbs\",\n",
    "                        \"sum_granted_prbs\",\n",
    "                        \"empty_5\",\n",
    "                        \"dl_pmi\",\n",
    "                        \"dl_ri\",\n",
    "                        \"ul_n\",\n",
    "                        \"ul_turbo_iters\"]\n",
    "\n",
    "    # Column names we need to extract from the dataset\n",
    "    metric_list_to_extract = [\"slice_id\",\n",
    "                              \"dl_buffer [bytes]\",\n",
    "                              \"tx_brate downlink [Mbps]\",\n",
    "                              \"sum_requested_prbs\",\n",
    "                              \"sum_granted_prbs\"]\n",
    "\n",
    "    # configure logger and console output\n",
    "    logging.basicConfig(level=logging.DEBUG, filename='./preprocessing.log', filemode='a+',\n",
    "                        format='%(asctime)-15s %(levelname)-8s %(message)s')\n",
    "    formatter = logging.Formatter('%(asctime)-15s %(levelname)-8s %(message)s')\n",
    "    console = logging.StreamHandler()\n",
    "    console.setLevel(logging.INFO)\n",
    "    console.setFormatter(formatter)\n",
    "    logging.getLogger('').addHandler(console)\n",
    "\n",
    "    use_gpu_in_env = True\n",
    "    mtc_policy_filename = './ml_models/mtc_policy'\n",
    "    urllc_policy_filename = './ml_models/urllc_policy'\n",
    "    embb_policy_filename = './ml_models/embb_policy'\n",
    "    autoencoder_filename = './ml_models/encoder.h5'\n",
    "\n",
    "    # Location of the dataset we want to use (valid in offline testing ONLY)\n",
    "    main_folder = './slice_traffic/rome_static_close/tr10'\n",
    "    wildcard_match = '/*/*/slices_bs*/*_metrics.csv'\n",
    "\n",
    "    # get dataset for testing purposes only.\n",
    "    # This is used as this code does not run with hardware components.\n",
    "    # Not needed if getting data from real DUs\n",
    "    dataset = entire_dataset_from_folder(main_folder=main_folder,\n",
    "                                         wildcard=wildcard_match,\n",
    "                                         col_names=all_metrics_list,\n",
    "                                         selected_col_names=metric_list_to_extract)\n",
    "\n",
    "    # Input size to the autoencoder for dimentionality reduction\n",
    "    n_entries_for_autoencoder = 10\n",
    "\n",
    "    # set logging level + enable TF2 behavior\n",
    "    absl.logging.set_verbosity(absl.logging.INFO)\n",
    "    # select which GPU to use\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "    if use_gpu_in_env is False:\n",
    "        gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "        tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "        print(\"Num GPUs Available outside environments: \", len(gpu_devices))\n",
    "\n",
    "    # load policy, these are the folder where saved_model.pb is stored\n",
    "    drl_agents = [tf.saved_model.load(embb_policy_filename),\n",
    "                  tf.saved_model.load(mtc_policy_filename),\n",
    "                  tf.saved_model.load(urllc_policy_filename)]\n",
    "\n",
    "    absl.logging.info('Agents loaded')\n",
    "\n",
    "    autoencoder = tf.keras.models.load_model(autoencoder_filename)\n",
    "\n",
    "    absl.logging.info('Autoencoder loaded')\n",
    "\n",
    "    slice_profiles = {'embb': {'slice_id': 0, 'reward_metric': \"tx_brate downlink [Mbps]\"},\n",
    "                      'mtc': {'slice_id': 1, 'reward_metric': \"tx_brate downlink [Mbps]\"},\n",
    "                      'urllc': {'slice_id': 2, 'reward_metric': \"ratio_granted_req\"}}\n",
    "\n",
    "    metric_dict = {\"dl_buffer [bytes]\": 1,\n",
    "                   \"tx_brate downlink [Mbps]\": 2,\n",
    "                   \"ratio_granted_req\": 3,\n",
    "                   \"slice_id\": 0,\n",
    "                   \"slice_prb\": 4}\n",
    "\n",
    "    metric_list_for_agents = [\"dl_buffer [bytes]\",\n",
    "                   \"tx_brate downlink [Mbps]\",\n",
    "                   \"ratio_granted_req\"]\n",
    "\n",
    "    default_policy = 0\n",
    "    previous_policy = dict()\n",
    "    for _, val in slice_profiles.items():\n",
    "        previous_policy[val['slice_id']] = default_policy\n",
    "\n",
    "    previous_metrics = ''\n",
    "\n",
    "    while True:\n",
    "        policies = list()\n",
    "\n",
    "        # This is where data comes from the DUs.\n",
    "        # As an example, we extract data from the static dataset.\n",
    "        # Users may want to interface it with their own DUs\n",
    "        data = get_data_from_DUs(dataset=dataset,\n",
    "                                 n_entries=1000,\n",
    "                                 metrics_export=metric_list_to_extract).to_numpy()\n",
    "\n",
    "        data_tmp, prbs, rewards = split_data(slice_profiles=slice_profiles,\n",
    "                                             data_to_spit=data,\n",
    "                                             metric_dict=metric_dict,\n",
    "                                             metric_list=metric_list_for_agents,\n",
    "                                             n_entries_per_slice=n_entries_for_autoencoder)\n",
    "\n",
    "        for i in range(len(slice_profiles)):\n",
    "            if len(data_tmp[i]) > 0:\n",
    "                for row in data_tmp[i]:\n",
    "                    row[0] /= 100000\n",
    "\n",
    "                logging.info('Testing iteration ' + str(i))\n",
    "                logging.info('Data received from DU (dl_buffer [bytes], tx_brate downlink [Mbps], ratio_granted_req): ')\n",
    "                logging.info(np.expand_dims(data_tmp[i], axis=0))\n",
    "\n",
    "                obs_tmp = autoencoder.predict(np.expand_dims(data_tmp[i], axis=0)).astype('float32')\n",
    "                obs_tmp = np.append(obs_tmp, prbs[i][0]).astype('float32')\n",
    "\n",
    "                reward_mean = np.mean(rewards[i]).astype('float32')\n",
    "                time_step = generate_timestep_for_policy(obs_tmp)\n",
    "                action = drl_agents[i].action(time_step)\n",
    "\n",
    "                # append policies to send and store policy\n",
    "                policies.append(action[0][0][0].numpy())\n",
    "                previous_policy[i] = action[0][0][0].numpy()\n",
    "\n",
    "                logging.info('Slice ' + str(i) + ': Action is ' + str(action[0][0][0].numpy()) + ' Reward is: ' + str(\n",
    "                    reward_mean))\n",
    "            else:\n",
    "                # append previous policy\n",
    "                policies.append(previous_policy[i])\n",
    "                logging.info('Using previous action ' + str(previous_policy[i]) + ' for slice profile ' + str(i))\n",
    "\n",
    "        # build message to send policies to the DU\n",
    "        msg = ','.join([str(x) for x in policies])\n",
    "        logging.info('Sending this message to the DU: ' + msg)\n",
    "\n",
    "        time.sleep(10)\n",
    "\n",
    "        # Users may want to plug in their own functions to send the DRL policies\n",
    "        # to the DU based on the specific DU implementation in use\n",
    "        # send_action_to_DU(DU_address, msg)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
