{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18784,"status":"ok","timestamp":1680747184594,"user":{"displayName":"유현민","userId":"10604916272596560835"},"user_tz":-540},"id":"NeJDJ0ffRQuH","outputId":"82aab25f-1deb-4574-f155-e3ededcf852a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/My Drive/Colab_Notebooks')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CGaMJZXH3WqY"},"outputs":[],"source":["#!git clone https://github.com/wineslab/colosseum-oran-commag-dataset.git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1680743506743,"user":{"displayName":"유현민","userId":"10604916272596560835"},"user_tz":-540},"id":"Z75LsHgF2Nhj","outputId":"e97ae402-c47c-4426-afb9-d1e6ff5a24de"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/Colab_Notebooks'"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["pwd"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":305,"status":"ok","timestamp":1680747186704,"user":{"displayName":"유현민","userId":"10604916272596560835"},"user_tz":-540},"id":"liQCxLXOTB0_","outputId":"0a33cf9c-be38-4907-ae57-3ff0fad41d04"},"outputs":[{"name":"stdout","output_type":"stream","text":["Thu Apr 20 23:08:09 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 528.02       Driver Version: 528.02       CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA GeForce ... WDDM  | 00000000:26:00.0  On |                  N/A |\n","|  0%   32C    P5    25W / 270W |   1254MiB /  8192MiB |     20%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|    0   N/A  N/A      4032    C+G   ...Roaming\\Zoom\\bin\\Zoom.exe    N/A      |\n","|    0   N/A  N/A     10476    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n","|    0   N/A  N/A     10576    C+G   ...ekyb3d8bbwe\\HxOutlook.exe    N/A      |\n","|    0   N/A  N/A     12264    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n","|    0   N/A  N/A     16532    C+G   ...oft\\OneDrive\\OneDrive.exe    N/A      |\n","|    0   N/A  N/A     16908    C+G   ...ge\\Application\\msedge.exe    N/A      |\n","|    0   N/A  N/A     17404    C+G   ...j98g3asy\\SecondScreen.exe    N/A      |\n","|    0   N/A  N/A     18048    C+G   ...3.0.4.0\\GoogleDriveFS.exe    N/A      |\n","|    0   N/A  N/A     19676    C+G   ...er_engine\\wallpaper64.exe    N/A      |\n","|    0   N/A  N/A     19952    C+G   ...ram Files\\LGHUB\\lghub.exe    N/A      |\n","|    0   N/A  N/A     20272    C+G   ...n64\\EpicGamesLauncher.exe    N/A      |\n","|    0   N/A  N/A     22024    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n","|    0   N/A  N/A     22088    C+G   ...artMenuExperienceHost.exe    N/A      |\n","|    0   N/A  N/A     22644    C+G   ...8wekyb3d8bbwe\\Cortana.exe    N/A      |\n","|    0   N/A  N/A     23008    C+G   ...EarTrumpet\\EarTrumpet.exe    N/A      |\n","|    0   N/A  N/A     23204    C+G   ...batNotificationClient.exe    N/A      |\n","|    0   N/A  N/A     24460    C+G   ...me\\Application\\chrome.exe    N/A      |\n","|    0   N/A  N/A     24776    C+G   ...8bbwe\\Notepad\\Notepad.exe    N/A      |\n","|    0   N/A  N/A     25716    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n","|    0   N/A  N/A     29244    C+G   ...obeNotificationClient.exe    N/A      |\n","|    0   N/A  N/A     29452    C+G   ...722.39\\msedgewebview2.exe    N/A      |\n","|    0   N/A  N/A     30132    C+G   ...\\app-1.0.9012\\Discord.exe    N/A      |\n","|    0   N/A  N/A     31084    C+G   ...n1h2txyewy\\SearchHost.exe    N/A      |\n","|    0   N/A  N/A     32232    C+G   C:\\Windows\\explorer.exe         N/A      |\n","|    0   N/A  N/A     36208    C+G   ...722.48\\msedgewebview2.exe    N/A      |\n","|    0   N/A  N/A     37868    C+G   ...a0\\XboxGameBarSpotify.exe    N/A      |\n","|    0   N/A  N/A     38332    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n","|    0   N/A  N/A     45584    C+G   ...s\\Win64\\EpicWebHelper.exe    N/A      |\n","|    0   N/A  N/A     48732    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n","|    0   N/A  N/A     49224    C+G   ...ray\\lghub_system_tray.exe    N/A      |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1680747188845,"user":{"displayName":"유현민","userId":"10604916272596560835"},"user_tz":-540},"id":"CseqVAn_TCUD","outputId":"38cee28e-e08f-4bcd-fb30-4f81c99290bd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Your runtime has 13.6 gigabytes of available RAM\n","\n","Not using a high-RAM runtime\n"]}],"source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17310,"status":"ok","timestamp":1680747208134,"user":{"displayName":"유현민","userId":"10604916272596560835"},"user_tz":-540},"id":"PnJeQox8RQlm","outputId":"a1c321fc-1bdf-4cf3-8acc-7fde36c693ab"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tf_agents\n","  Downloading tf_agents-0.16.0-py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from tf_agents) (4.5.0)\n","Requirement already satisfied: tensorflow-probability~=0.19.0 in /usr/local/lib/python3.9/dist-packages (from tf_agents) (0.19.0)\n","Collecting pygame==2.1.3\n","  Downloading pygame-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gym<=0.23.0,>=0.17.0\n","  Downloading gym-0.23.0.tar.gz (624 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 KB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.9/dist-packages (from tf_agents) (1.14.1)\n","Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.9/dist-packages (from tf_agents) (1.4.0)\n","Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from tf_agents) (0.5.0)\n","Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.9/dist-packages (from tf_agents) (2.2.1)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from tf_agents) (1.16.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.9/dist-packages (from tf_agents) (1.22.4)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from tf_agents) (8.4.0)\n","Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.9/dist-packages (from tf_agents) (3.20.3)\n","Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.9/dist-packages (from gym<=0.23.0,>=0.17.0->tf_agents) (6.1.0)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym<=0.23.0,>=0.17.0->tf_agents) (0.0.8)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability~=0.19.0->tf_agents) (4.4.2)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability~=0.19.0->tf_agents) (0.1.8)\n","Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow-probability~=0.19.0->tf_agents) (0.4.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.10.0->gym<=0.23.0,>=0.17.0->tf_agents) (3.15.0)\n","Building wheels for collected packages: gym\n","  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697658 sha256=9c962f4503fd9ac8248222219a48302ff55407e4b451e12b91b3b9645f40f3f8\n","  Stored in directory: /root/.cache/pip/wheels/96/b9/bb/994c1324b65e39dd1cd7b8ba92e5fb766dd77980929414a866\n","Successfully built gym\n","Installing collected packages: pygame, gym, tf_agents\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.3.0\n","    Uninstalling pygame-2.3.0:\n","      Successfully uninstalled pygame-2.3.0\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.25.2\n","    Uninstalling gym-0.25.2:\n","      Successfully uninstalled gym-0.25.2\n","Successfully installed gym-0.23.0 pygame-2.1.3 tf_agents-0.16.0\n"]}],"source":["!pip install tf_agents"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":458},"executionInfo":{"elapsed":15739,"status":"ok","timestamp":1679383380051,"user":{"displayName":"유현민","userId":"10604916272596560835"},"user_tz":-540},"id":"YWWvpfgGDCJ1","outputId":"742922f0-94ef-4015-8e02-8f1e67024793"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found existing installation: numpy 1.24.2\n","Uninstalling numpy-1.24.2:\n","  Would remove:\n","    /usr/local/bin/f2py\n","    /usr/local/bin/f2py3\n","    /usr/local/bin/f2py3.9\n","    /usr/local/lib/python3.9/dist-packages/numpy-1.24.2.dist-info/*\n","    /usr/local/lib/python3.9/dist-packages/numpy.libs/libgfortran-040039e1.so.5.0.0\n","    /usr/local/lib/python3.9/dist-packages/numpy.libs/libopenblas64_p-r0-15028c96.3.21.so\n","    /usr/local/lib/python3.9/dist-packages/numpy.libs/libquadmath-96973f99.so.0.0.0\n","    /usr/local/lib/python3.9/dist-packages/numpy/*\n","Proceed (Y/n)? y\n","  Successfully uninstalled numpy-1.24.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting numpy==1.23.1\n","  Downloading numpy-1.23.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy\n","Successfully installed numpy-1.23.1\n"]},{"data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]}}},"metadata":{},"output_type":"display_data"}],"source":["#TPU 사용할 때\n","!pip uninstall numpy\n","!pip install numpy==1.23.1"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"bCHuUPpZQ_tZ","outputId":"a295167e-8363-479d-ce8f-f5fd2049ba9b"},"outputs":[{"ename":"ValueError","evalue":"No objects to concatenate","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[2], line 267\u001b[0m\n\u001b[0;32m    263\u001b[0m wildcard_match \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/*/*/slices_bs*/*_metrics.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    264\u001b[0m \u001b[39m# get dataset for testing purposes only.\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[39m# This is used as this code does not run with hardware components.\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[39m# Not needed if getting data from real DUs\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m dataset \u001b[39m=\u001b[39m entire_dataset_from_folder(main_folder\u001b[39m=\u001b[39;49mmain_folder,\n\u001b[0;32m    268\u001b[0m                                      wildcard\u001b[39m=\u001b[39;49mwildcard_match,\n\u001b[0;32m    269\u001b[0m                                      col_names\u001b[39m=\u001b[39;49mall_metrics_list,\n\u001b[0;32m    270\u001b[0m                                      selected_col_names\u001b[39m=\u001b[39;49mmetric_list_to_extract)\n\u001b[0;32m    271\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdataset : \u001b[39m\u001b[39m\"\u001b[39m, dataset)\n\u001b[0;32m    272\u001b[0m \u001b[39m# Input size to the autoencoder for dimentionality reduction\u001b[39;00m\n","Cell \u001b[1;32mIn[2], line 80\u001b[0m, in \u001b[0;36mentire_dataset_from_folder\u001b[1;34m(main_folder, wildcard, col_names, selected_col_names, scale_dl_buffer, remove_zero_req_prb_entries, replace_zero_with_one, add_prb_ratio)\u001b[0m\n\u001b[0;32m     71\u001b[0m     db_tmp \u001b[39m=\u001b[39m entire_dataset_from_single_file(filename, col_names\u001b[39m=\u001b[39mcol_names,\n\u001b[0;32m     72\u001b[0m                                              selected_col_names\u001b[39m=\u001b[39mselected_col_names,\n\u001b[0;32m     73\u001b[0m                                              scale_dl_buffer\u001b[39m=\u001b[39mscale_dl_buffer,\n\u001b[0;32m     74\u001b[0m                                              remove_zero_req_prb_entries\u001b[39m=\u001b[39mremove_zero_req_prb_entries,\n\u001b[0;32m     75\u001b[0m                                              replace_zero_with_one\u001b[39m=\u001b[39mreplace_zero_with_one,\n\u001b[0;32m     76\u001b[0m                                              add_prb_ratio\u001b[39m=\u001b[39madd_prb_ratio)\n\u001b[0;32m     78\u001b[0m     dataset\u001b[39m.\u001b[39mappend(db_tmp)\n\u001b[1;32m---> 80\u001b[0m \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39;49mconcat(dataset, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, ignore_index\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\reshape\\concat.py:372\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[39melif\u001b[39;00m copy \u001b[39mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    370\u001b[0m     copy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 372\u001b[0m op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[0;32m    373\u001b[0m     objs,\n\u001b[0;32m    374\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m    375\u001b[0m     ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[0;32m    376\u001b[0m     join\u001b[39m=\u001b[39;49mjoin,\n\u001b[0;32m    377\u001b[0m     keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[0;32m    378\u001b[0m     levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[0;32m    379\u001b[0m     names\u001b[39m=\u001b[39;49mnames,\n\u001b[0;32m    380\u001b[0m     verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[0;32m    381\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    382\u001b[0m     sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m    383\u001b[0m )\n\u001b[0;32m    385\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\reshape\\concat.py:429\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    426\u001b[0m     objs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(objs)\n\u001b[0;32m    428\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(objs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 429\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo objects to concatenate\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    431\u001b[0m \u001b[39mif\u001b[39;00m keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    432\u001b[0m     objs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(com\u001b[39m.\u001b[39mnot_none(\u001b[39m*\u001b[39mobjs))\n","\u001b[1;31mValueError\u001b[0m: No objects to concatenate"]}],"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import tensorflow as tf\n","from tf_agents.trajectories import time_step as ts\n","\n","import absl\n","import time\n","import os\n","import glob\n","import sys\n","\n","\n","import pandas as pd\n","import numpy as np\n","\n","import logging\n","\n","sys.stdout = open('log.txt', 'w')\n","\n","# Generating dataset from csv file. Returns a Pandas DataFrame\n","def entire_dataset_from_single_file(filename,\n","                                    col_names, # all_metrics_list\n","                                    selected_col_names, # metric_list_to_extract = [\"slice_id\", \"dl_buffer [bytes]\", \"tx_brate downlink [Mbps]\", \"sum_requested_prbs\", \"sum_granted_prbs\"]\n","                                    remove_zero_req_prb_entries=True,\n","                                    scale_dl_buffer=True,\n","                                    replace_zero_with_one=False,\n","                                    add_prb_ratio=True):\n","    dataset = pd.read_csv(filename, names=col_names, usecols=selected_col_names, header=0)\n","    print(\"-------------------------------------------- entire_dataset_from_single_file 시작 --------------------------------------------\")\n","    print(\"filename : \", filename)\n","    print(\"이전 dataset : \", dataset)\n","    if remove_zero_req_prb_entries:\n","        dataset = dataset.loc[dataset['sum_requested_prbs'] > 0].reset_index(drop=True)\n","    print(\"remove_zero_req_prb_entries 이후 dataset : \", dataset)\n","    \n","    if scale_dl_buffer and any([\"dl_buffer [bytes]\" in m for m in # in: 데이터 안에 찾고자 하는 것이 있는지 확인하는 연산자\n","                                selected_col_names]):\n","        # scale the dl_buffer\n","        dataset['dl_buffer [bytes]'] = dataset['dl_buffer [bytes]'] / 100000\n","    \n","    print(\"scale_dl_buffer 이후 dataset : \", dataset)\n","\n","    if add_prb_ratio:\n","        dict_add = pd.DataFrame.from_dict({\"ratio_granted_req\": np.clip(np.nan_to_num(  # from_dict :  dict객체로부터 DataFrame 객체로 변환하는 메서드\n","            dataset[\"sum_granted_prbs\"] / dataset[\"sum_requested_prbs\"]), a_min=0, a_max=1) # numpy.clip(array, min, max)\n","        })\n","        print(\"dict_add : \", dict_add)\n","        print(\"-------------------------------------------- entire_dataset_from_single_file 끝 --------------------------------------------\")\n","\n","        if replace_zero_with_one: # 실행 안됨\n","            print(\"replace_zero_with_one 실행 : \")\n","            dict_add['ratio_granted_req'].loc[dataset['sum_requested_prbs'] <= 0] = 1.0\n","        return dataset.join(dict_add)\n","    else:\n","        return dataset\n","\n","# return all csv files inside a single DataFrame\n","def entire_dataset_from_folder(main_folder,\n","                               wildcard,\n","                               col_names, # all_metrics_list\n","                               selected_col_names, # metric_list_to_extract = [\"slice_id\", \"dl_buffer [bytes]\", \"tx_brate downlink [Mbps]\", \"sum_requested_prbs\", \"sum_granted_prbs\"]\n","                               scale_dl_buffer=True,\n","                               remove_zero_req_prb_entries=True,\n","                               replace_zero_with_one=False,\n","                               add_prb_ratio=True):\n","  \n","    dataset = []\n","    for filename in glob.glob(main_folder + wildcard):\n","        db_tmp = entire_dataset_from_single_file(filename, col_names=col_names,\n","                                                 selected_col_names=selected_col_names,\n","                                                 scale_dl_buffer=scale_dl_buffer,\n","                                                 remove_zero_req_prb_entries=remove_zero_req_prb_entries,\n","                                                 replace_zero_with_one=replace_zero_with_one,\n","                                                 add_prb_ratio=add_prb_ratio)\n","                                                 \n","        dataset.append(db_tmp)\n","    \n","    return pd.concat(dataset, axis=0, ignore_index=True)\n","\n","\n","# take n entries from the DataFrame at random\n","def extract_n_entries_from_dataset(dataset=None,\n","                                   slice_id=None,\n","                                   n_entries=10,\n","                                   metrics_export=None):\n","    if slice_id is not None:\n","        d_temp = dataset.loc[dataset['slice_id'] == int(slice_id)]\n","    else:\n","        d_temp = dataset\n","\n","    d_temp = d_temp.sample(n=n_entries).reset_index(drop=True)\n","    if metrics_export is not None:\n","        d_temp = d_temp[metrics_export]\n","\n","    return d_temp\n","\n","# This function is used here to emulate a DU reporting real-time data. Replace this function with your DU\n","# FOR TESTING PURPOSES ONLY\n","def get_data_from_DUs(dataset=None,\n","                      n_entries=1000,\n","                      n_col=4,\n","                      slice_id=None,\n","                      metrics_export=None):\n","\n","    if dataset is None:  # generate random data in case you do not have a dataset\n","        values = np.random.random(size=(n_entries, n_col))\n","        slice_id = np.random.randint(low=0, high=3, size=(n_entries, 1))\n","        data = np.concatenate((slice_id, values), axis=1)\n","    else:\n","        data = extract_n_entries_from_dataset(dataset=dataset,\n","                                              slice_id=slice_id,\n","                                              n_entries=n_entries,\n","                                              metrics_export=metrics_export)\n","\n","    return data\n","\n","\n","# Return lists for metrics, rewards, prbs assigned to each slice.\n","# Ideally, the list is such that len(list) = num_slices\n","def split_data(slice_profiles=None, # slice_profiles\n","               data_to_spit=None, # data_to_spit=data\n","               metric_list=None, # metric_dict=metric_dict\n","               metric_dict=None, # metric_list=metric_list_for_agents\n","               n_entries_per_slice=None): # n_entries_per_slice=n_entries_for_autoencoder\n","    print(\"----------------------------------------split_data 시작----------------------------------------\")\n","    print(\"slice_profiles : \", slice_profiles)\n","    print(\"data_to_spit : \", data_to_spit)\n","    print(\"metric_list : \", metric_list)\n","    print(\"metric_dict : \", metric_dict)\n","    print(\"n_entries_per_slice : \", n_entries_per_slice)\n","    \n","    \n","    metrics = []\n","    prbs = []\n","    rewards = []\n","    \n","    # ordering here follows slice_profiles\n","    for i in slice_profiles:\n","\n","        slice_data = data_to_spit[data_to_spit[:, metric_dict['slice_id']] == slice_profiles[i]['slice_id'], :]\n","        print(\"slice_data\")\n","        if slice_data.size > 0:\n","            # repmat on rows to reach needed dimension in case you do not have enough reporting data\n","            while slice_data.shape[0] < n_entries_per_slice:\n","                slice_data = np.vstack((slice_data, np.zeros((1, slice_data.shape[1]))))\n","                print(\"slice_data in while :\", slice_data)\n","            print(\"slice_data out of while :\", slice_data)\n","\n","            slice_prb = slice_data[:, metric_dict['slice_prb']]\n","            slice_metrics = slice_data[:, [metric_dict[x] for x in metric_list]]\n","            slice_reward = slice_data[:, metric_dict[slice_profiles[i]['reward_metric']]]\n","            print(\"slice_prb : \", slice_prb)\n","            print(\"slice_metrics : \", slice_metrics)\n","            print(\"slice_reward : \", slice_reward)\n","            \n","\n","            if n_entries_per_slice is not None:\n","                slice_prb = slice_prb[0:n_entries_per_slice]\n","                slice_metrics = slice_metrics[0:n_entries_per_slice, :]\n","                slice_reward = slice_reward[0:n_entries_per_slice]\n","                print(\"if문 slice_prb : \", slice_prb)\n","                print(\"if문 slice_metrics : \", slice_metrics)\n","                print(\"if문 slice_reward : \", slice_reward)\n","        else:\n","            slice_metrics = []\n","            slice_prb = []\n","            slice_reward = []\n","\n","        metrics.append(slice_metrics)\n","        prbs.append(slice_prb)\n","        rewards.append(slice_reward)\n","        print(\"metrics : \", metrics)\n","        print(\"prbs : \", prbs)\n","        print(\"rewards : \", rewards)\n","        \n","    print(\"----------------------------------------split_data 끝----------------------------------------\")\n","    return metrics, prbs, rewards\n","\n","# Used to generate the input to the DRL agent. It returns a TimeStep that contains (step_type, reward, discount, observations)\n","def generate_timestep_for_policy(obs_tmp=None):\n","    print(\"-------------------------------generate_timestep_for_policy 시작-------------------------------\")\n","    step_type = tf.convert_to_tensor(\n","        [0], dtype=tf.int32, name='step_type')\n","    print(\"step_type : \", step_type)\n","    reward = tf.convert_to_tensor(\n","        [0], dtype=tf.float32, name='reward')\n","    print(\"reward : \", reward)\n","    discount = tf.convert_to_tensor(\n","        [1], dtype=tf.float32, name='discount')\n","    print(\"discount : \", discount)\n","    observations = tf.convert_to_tensor(\n","        [obs_tmp], dtype=tf.float32, name='observations')\n","    print(\"observations : \", observations)\n","    print(\"-------------------------------generate_timestep_for_policy 끝-------------------------------\")\n","    return ts.TimeStep(step_type, reward, discount, observations)\n","\n","if __name__ == '__main__':\n","    print(\"----------------------------main함수 시작----------------------------\")\n","    # Column names in the srsLTE CSV dataset\n","    all_metrics_list = [\"Timestamp\",\n","                        \"num_ues\",\n","                        \"IMSI\",\n","                        \"RNTI\",\n","                        \"empty_1\",\n","                        \"slicing_enabled\",\n","                        \"slice_id\",\n","                        \"slice_prb\",\n","                        \"power_multiplier\",\n","                        \"scheduling_policy\",\n","                        \"empty_2\",\n","                        \"dl_mcs\",\n","                        \"dl_n_samples\",\n","                        \"dl_buffer [bytes]\",\n","                        \"tx_brate downlink [Mbps]\",\n","                        \"tx_pkts downlink\",\n","                        \"tx_errors downlink (%)\",\n","                        \"dl_cqi\",\n","                        \"empty_3\",\n","                        \"ul_mcs\",\n","                        \"ul_n_samples\",\n","                        \"ul_buffer [bytes]\",\n","                        \"rx_brate uplink [Mbps]\",\n","                        \"rx_pkts uplink\",\n","                        \"rx_errors uplink (%)\",\n","                        \"ul_rssi\",\n","                        \"ul_sinr\",\n","                        \"phr\",\n","                        \"empty_4\",\n","                        \"sum_requested_prbs\",\n","                        \"sum_granted_prbs\",\n","                        \"empty_5\",\n","                        \"dl_pmi\",\n","                        \"dl_ri\",\n","                        \"ul_n\",\n","                        \"ul_turbo_iters\"]\n","\n","    # Column names we need to extract from the dataset\n","    metric_list_to_extract = [\"slice_id\",\n","                              \"dl_buffer [bytes]\",\n","                              \"tx_brate downlink [Mbps]\",\n","                              \"sum_requested_prbs\",\n","                              \"sum_granted_prbs\"]\n","\n","    # configure logger and console output\n","    logging.basicConfig(level=logging.DEBUG, filename='./agent.log', filemode='a+',\n","                        format='%(asctime)-15s %(levelname)-8s %(message)s')\n","    formatter = logging.Formatter('%(asctime)-15s %(levelname)-8s %(message)s')\n","    console = logging.StreamHandler() #FileHandler('./agent.log')\n","    console.setLevel(logging.INFO)\n","    console.setFormatter(formatter)\n","    logging.getLogger('').addHandler(console)\n","\n","    use_gpu_in_env = True\n","    mtc_policy_filename = './ml_models/mtc_policy'\n","    urllc_policy_filename = './ml_models/urllc_policy'\n","    embb_policy_filename = './ml_models/embb_policy'\n","    autoencoder_filename = './ml_models/encoder.h5'\n","\n","    # Location of the dataset we want to use (valid in offline testing ONLY)\n","    main_folder = './slice_traffic/rome_static_close/tr10'\n","    wildcard_match = '/*/*/slices_bs*/*_metrics.csv'\n","    # get dataset for testing purposes only.\n","    # This is used as this code does not run with hardware components.\n","    # Not needed if getting data from real DUs\n","    dataset = entire_dataset_from_folder(main_folder=main_folder,\n","                                         wildcard=wildcard_match,\n","                                         col_names=all_metrics_list,\n","                                         selected_col_names=metric_list_to_extract)\n","    print(\"dataset : \", dataset)\n","    # Input size to the autoencoder for dimentionality reduction\n","    n_entries_for_autoencoder = 10\n","\n","    # set logging level + enable TF2 behavior\n","    absl.logging.set_verbosity(absl.logging.INFO)\n","    # select which GPU to use\n","    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","\n","    if use_gpu_in_env is False:\n","        gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n","        tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n","        print(\"Num GPUs Available outside environments: \", len(gpu_devices))\n","\n","    # load policy, these are the folder where saved_model.pb is stored\n","    drl_agents = [tf.saved_model.load(embb_policy_filename),\n","                  tf.saved_model.load(mtc_policy_filename),\n","                  tf.saved_model.load(urllc_policy_filename)]\n","    print(\"drl_agents : \", drl_agents)\n","    absl.logging.info('Agents loaded')\n","\n","    autoencoder = tf.keras.models.load_model(autoencoder_filename, compile=False)\n","\n","    absl.logging.info('Autoencoder loaded')\n","\n","    slice_profiles = {'embb': {'slice_id': 0, 'reward_metric': \"tx_brate downlink [Mbps]\"},\n","                      'mtc': {'slice_id': 1, 'reward_metric': \"tx_brate downlink [Mbps]\"},\n","                      'urllc': {'slice_id': 2, 'reward_metric': \"ratio_granted_req\"}}\n","\n","    metric_dict = {\"dl_buffer [bytes]\": 1,\n","                   \"tx_brate downlink [Mbps]\": 2,\n","                   \"ratio_granted_req\": 3,\n","                   \"slice_id\": 0,\n","                   \"slice_prb\": 4}\n","\n","    metric_list_for_agents = [\"dl_buffer [bytes]\",\n","                   \"tx_brate downlink [Mbps]\",\n","                   \"ratio_granted_req\"]\n","\n","    default_policy = 0\n","    previous_policy = dict()\n","    for _, val in slice_profiles.items():\n","        previous_policy[val['slice_id']] = default_policy\n","    print(\"previous_policy : \", previous_policy)\n","\n","    previous_metrics = ''\n","    print(\"----------------------------main함수끝----------------------------\")\n","\n","    while True:\n","        print(\"------------------------------------------------while 시작------------------------------------------------\")\n","        policies = list()\n","\n","        # This is where data comes from the DUs.\n","        # As an example, we extract data from the static dataset.\n","        # Users may want to interface it with their own DUs\n","        data = get_data_from_DUs(dataset=dataset,\n","                                 n_entries=1000,\n","                                 metrics_export=metric_list_to_extract).to_numpy()\n","        print(\"data : \", data)\n","\n","        data_tmp, prbs, rewards = split_data(slice_profiles=slice_profiles,\n","                                             data_to_spit=data,\n","                                             metric_dict=metric_dict,\n","                                             metric_list=metric_list_for_agents,\n","                                             n_entries_per_slice=n_entries_for_autoencoder)\n","        print(\"data_tmp : \", data_tmp, \"prbs : \", prbs, \"rewards : \", rewards)\n","        print(\"slice_profiles : \", slice_profiles)\n","        for i in range(len(slice_profiles)):\n","            print(\"len(slice_profiles) : \", len(slice_profiles)) #-> 3\n","            if len(data_tmp[i]) > 0:\n","                print(\"len(data_tmp[i]) :\", len(data_tmp[i])) #-> 10\n","                for row in data_tmp[i]:\n","                    row[0] /= 100000\n","\n","                logging.info('Testing iteration ' + str(i))\n","                print(\"Testing iteration \" + str(i))\n","                logging.info('Data received from DU (dl_buffer [bytes], tx_brate downlink [Mbps], ratio_granted_req): ')\n","                print(\"Data received from DU (dl_buffer [bytes], tx_brate downlink [Mbps], ratio_granted_req): \")\n","                print(\"np.expand_dims(data_tmp[i], axis=0) : \", np.expand_dims(data_tmp[i], axis=0))\n","                logging.info(np.expand_dims(data_tmp[i], axis=0))\n","\n","                obs_tmp = autoencoder.predict(np.expand_dims(data_tmp[i], axis=0)).astype('float32')\n","                print(\"obs_tmp 1: \", obs_tmp)\n","                print(\"prbs[\", i, \"][0] : \", prbs[i][0])\n","                obs_tmp = np.append(obs_tmp, prbs[i][0]).astype('float32')\n","                print(\"obs_tmp 2: \", obs_tmp)\n","\n","                print(\"rewards[\", i, \"] : \", rewards[i])\n","                reward_mean = np.mean(rewards[i]).astype('float32')\n","                print(\"reward_mean : \", reward_mean)\n","                time_step = generate_timestep_for_policy(obs_tmp)\n","                print(\"time_step \", time_step)\n","                action = drl_agents[i].action(time_step)\n","                print(\"action \", action)\n","\n","                # append policies to send and store policy\n","                print(\"action[0][0][0].numpy() : \", action[0][0][0].numpy())\n","                policies.append(action[0][0][0].numpy())\n","                previous_policy[i] = action[0][0][0].numpy()\n","\n","                logging.info('Slice ' + str(i) + ': Action is ' + str(action[0][0][0].numpy()) + ' Reward is: ' + str(reward_mean))\n","                print(\"Slice \"+ str(i) + \": Action is \" + str(action[0][0][0].numpy()) + \" Reward is: \" + str(reward_mean))\n","            else:\n","                # append previous policy\n","                policies.append(previous_policy[i])\n","                logging.info(\"Using previous action \" + str(previous_policy[i]) + \" for slice profile \" + str(i))\n","                print(\"Using previous action \" + str(previous_policy[i]) + \" for slice profile \" + str(i))\n","\n","        # build message to send policies to the DU\n","        msg = ','.join([str(x) for x in policies])\n","        logging.info(\"Sending this message to the DU: \" + msg)\n","        print(\"Sending this message to the DU: \" + msg)\n","        print(\"------------------------------------------------while 끝------------------------------------------------\")\n","\n","        time.sleep(10)\n","\n","        # Users may want to plug in their own functions to send the DRL policies\n","        # to the DU based on the specific DU implementation in use\n","        # send_action_to_DU(DU_address, msg)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C-pntEnRH0NJ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":841,"status":"ok","timestamp":1680057935076,"user":{"displayName":"유현민","userId":"10604916272596560835"},"user_tz":-540},"id":"tF7ninjMPowA","outputId":"15de9d57-c167-4cea-ce66-077adbf6a4ca"},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:absl:Interesting Stuff\n","2023-03-29 02:45:34,294 INFO     Interesting Stuff\n","2023-03-29 02:45:34,294 INFO     Interesting Stuff\n","2023-03-29 02:45:34,294 INFO     Interesting Stuff\n","2023-03-29 02:45:34,294 INFO     Interesting Stuff\n","2023-03-29 02:45:34,294 INFO     Interesting Stuff\n","INFO:absl:Interesting Stuff with Arguments: 42\n","2023-03-29 02:45:34,302 INFO     Interesting Stuff with Arguments: 42\n","2023-03-29 02:45:34,302 INFO     Interesting Stuff with Arguments: 42\n","2023-03-29 02:45:34,302 INFO     Interesting Stuff with Arguments: 42\n","2023-03-29 02:45:34,302 INFO     Interesting Stuff with Arguments: 42\n","2023-03-29 02:45:34,302 INFO     Interesting Stuff with Arguments: 42\n","Level 1:absl:This will *not* be printed\n","Level 1:absl:This will be printed\n","WARNING:absl:Worrying Stuff\n","2023-03-29 02:45:34,309 WARNING  Worrying Stuff\n","2023-03-29 02:45:34,309 WARNING  Worrying Stuff\n","2023-03-29 02:45:34,309 WARNING  Worrying Stuff\n","2023-03-29 02:45:34,309 WARNING  Worrying Stuff\n","2023-03-29 02:45:34,309 WARNING  Worrying Stuff\n","ERROR:absl:Alarming Stuff\n","2023-03-29 02:45:34,318 ERROR    Alarming Stuff\n","2023-03-29 02:45:34,318 ERROR    Alarming Stuff\n","2023-03-29 02:45:34,318 ERROR    Alarming Stuff\n","2023-03-29 02:45:34,318 ERROR    Alarming Stuff\n","2023-03-29 02:45:34,318 ERROR    Alarming Stuff\n","CRITICAL:absl:AAAAHHHHH!!!!\n","2023-03-29 02:45:34,326 CRITICAL AAAAHHHHH!!!!\n","2023-03-29 02:45:34,326 CRITICAL AAAAHHHHH!!!!\n","2023-03-29 02:45:34,326 CRITICAL AAAAHHHHH!!!!\n","2023-03-29 02:45:34,326 CRITICAL AAAAHHHHH!!!!\n","2023-03-29 02:45:34,326 CRITICAL AAAAHHHHH!!!!\n"]}],"source":["#from absl import logging\n","import logging\n","import absl\n","absl.logging.info('Interesting Stuff')\n","absl.logging.info('Interesting Stuff with Arguments: %d', 42)\n","\n","absl.logging.set_verbosity(logging.INFO)\n","absl.logging.log(logging.DEBUG, 'This will *not* be printed')\n","absl.logging.set_verbosity(logging.DEBUG)\n","absl.logging.log(logging.DEBUG, 'This will be printed')\n","\n","absl.logging.warning('Worrying Stuff')\n","absl.logging.error('Alarming Stuff')\n","absl.logging.fatal('AAAAHHHHH!!!!')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ud-Ep6G4NT2c"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNwJt7Zplcth5pjDBEthfoZ","machine_shape":"hm","provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
